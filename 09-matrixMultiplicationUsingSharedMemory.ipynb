{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5957d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc --version\n",
    "%pip install nvcc4jupyter\n",
    "%load_ext nvcc4jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0d40041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "  0.0   1.0   2.0   3.0   4.0   5.0   6.0   7.0 \n",
      "  8.0   9.0   0.0   1.0   2.0   3.0   4.0   5.0 \n",
      "  6.0   7.0   8.0   9.0   0.0   1.0   2.0   3.0 \n",
      "  4.0   5.0   6.0   7.0   8.0   9.0   0.0   1.0 \n",
      "  2.0   3.0   4.0   5.0   6.0   7.0   8.0   9.0 \n",
      "  0.0   1.0   2.0   3.0   4.0   5.0   6.0   7.0 \n",
      "  8.0   9.0   0.0   1.0   2.0   3.0   4.0   5.0 \n",
      "  6.0   7.0   8.0   9.0   0.0   1.0   2.0   3.0 \n",
      "\n",
      "B:\n",
      "  1.0   2.0   3.0   4.0   5.0   1.0   2.0   3.0 \n",
      "  4.0   5.0   1.0   2.0   3.0   4.0   5.0   1.0 \n",
      "  2.0   3.0   4.0   5.0   1.0   2.0   3.0   4.0 \n",
      "  5.0   1.0   2.0   3.0   4.0   5.0   1.0   2.0 \n",
      "  3.0   4.0   5.0   1.0   2.0   3.0   4.0   5.0 \n",
      "  1.0   2.0   3.0   4.0   5.0   1.0   2.0   3.0 \n",
      "  4.0   5.0   1.0   2.0   3.0   4.0   5.0   1.0 \n",
      "  2.0   3.0   4.0   5.0   1.0   2.0   3.0   4.0 \n",
      "\n",
      "C = A x B:\n",
      " 78.0  91.0  84.0  92.0  75.0  78.0  91.0  84.0 \n",
      " 84.0 111.0  78.0 100.0 107.0  84.0 111.0  78.0 \n",
      "110.0 101.0  92.0 128.0 109.0 110.0 101.0  92.0 \n",
      "106.0 111.0 126.0 126.0 131.0 106.0 111.0 126.0 \n",
      "122.0 141.0 130.0 144.0 123.0 122.0 141.0 130.0 \n",
      " 78.0  91.0  84.0  92.0  75.0  78.0  91.0  84.0 \n",
      " 84.0 111.0  78.0 100.0 107.0  84.0 111.0  78.0 \n",
      "110.0 101.0  92.0 128.0 109.0 110.0 101.0  92.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "\n",
    "#define N 8\n",
    "#define BLOCK_SIZE 4\n",
    "\n",
    "__global__ \n",
    "void matrixMulShared(float *A, float *B, float *C, int width) {\n",
    "    \n",
    "    __shared__ float tileA[BLOCK_SIZE][BLOCK_SIZE];\n",
    "    __shared__ float tileB[BLOCK_SIZE][BLOCK_SIZE];\n",
    "\n",
    "    \n",
    "    int row = blockIdx.y * BLOCK_SIZE + threadIdx.y;\n",
    "    int col = blockIdx.x * BLOCK_SIZE + threadIdx.x;\n",
    "\n",
    "    float value = 0.0f;\n",
    "\n",
    "    \n",
    "    for (int t = 0; t < width / BLOCK_SIZE; ++t) {\n",
    "        \n",
    "        if (row < width && t * BLOCK_SIZE + threadIdx.x < width)\n",
    "            tileA[threadIdx.y][threadIdx.x] = A[row * width + t * BLOCK_SIZE + threadIdx.x];\n",
    "        else\n",
    "            tileA[threadIdx.y][threadIdx.x] = 0;\n",
    "\n",
    "        if (t * BLOCK_SIZE + threadIdx.y < width && col < width)\n",
    "            tileB[threadIdx.y][threadIdx.x] = B[(t * BLOCK_SIZE + threadIdx.y) * width + col];\n",
    "        else\n",
    "            tileB[threadIdx.y][threadIdx.x] = 0;\n",
    "\n",
    "        __syncthreads();\n",
    "\n",
    "        for (int i = 0; i < BLOCK_SIZE; ++i)\n",
    "            value += tileA[threadIdx.y][i] * tileB[i][threadIdx.x];\n",
    "\n",
    "        __syncthreads();\n",
    "    }\n",
    "\n",
    "    if (row < width && col < width)\n",
    "        C[row * width + col] = value;\n",
    "}\n",
    "\n",
    "int main() {\n",
    "    int size = N * N;\n",
    "    size_t sz = size * sizeof(float);\n",
    "\n",
    "    float *h_A = (float *)malloc(sz);\n",
    "    float *h_B = (float *)malloc(sz);\n",
    "    float *h_C = (float *)malloc(sz);\n",
    "\n",
    "    for (int i = 0; i < size; ++i) {\n",
    "        h_A[i] = i % 10;\n",
    "        h_B[i] = (i % 5) + 1;\n",
    "    }\n",
    "\n",
    "    float *d_A, *d_B, *d_C;\n",
    "    cudaMalloc(&d_A, sz);\n",
    "    cudaMalloc(&d_B, sz);\n",
    "    cudaMalloc(&d_C, sz);\n",
    "\n",
    "    cudaMemcpy(d_A, h_A, sz, cudaMemcpyHostToDevice);\n",
    "    cudaMemcpy(d_B, h_B, sz, cudaMemcpyHostToDevice);\n",
    "\n",
    "    dim3 threads(BLOCK_SIZE, BLOCK_SIZE);\n",
    "    dim3 blocks((N + BLOCK_SIZE - 1) / BLOCK_SIZE, (N + BLOCK_SIZE - 1) / BLOCK_SIZE);\n",
    "\n",
    "    matrixMulShared<<<blocks, threads>>>(d_A, d_B, d_C, N);\n",
    "    cudaDeviceSynchronize();\n",
    "\n",
    "    cudaMemcpy(h_C, d_C, sz, cudaMemcpyDeviceToHost);\n",
    "\n",
    "    printf(\"A:\\n\");\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        for (int j = 0; j < N; j++)\n",
    "            printf(\"%5.1f \", h_A[i * N + j]);\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "\n",
    "    printf(\"\\nB:\\n\");\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        for (int j = 0; j < N; j++)\n",
    "            printf(\"%5.1f \", h_B[i * N + j]);\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "\n",
    "    printf(\"\\nC = A x B:\\n\");\n",
    "    for (int i = 0; i < N; i++) {\n",
    "        for (int j = 0; j < N; j++)\n",
    "            printf(\"%5.1f \", h_C[i * N + j]);\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "\n",
    "    cudaFree(d_A);\n",
    "    cudaFree(d_B);\n",
    "    cudaFree(d_C);\n",
    "    free(h_A);\n",
    "    free(h_B);\n",
    "    free(h_C);\n",
    "\n",
    "    return 0;\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
